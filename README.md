# How Not to Let Your Model and Data Drift Away Silently

Abstract:
ML deployment is not the end — it is where ML models start to materialize impact and value to the business and people. We need monitoring and testing to ensure ML models behave as expected.

What you’ll Learn:
A set of tests and metrics that ML practitioners should care about post ML deployment. 

**Important Note**:
- All tests chosen are non-parametric, which means they don't assume normality in data distributions
- For non-Databricks users, you will need to configure your own MLflow tracking server to view the MLflow UI 

Files in this repository:
- Slides in PDF
- Notebooks in both .dbc and .html formats
  - .dbc format is available to be uploaded to the Databricks environment
  - .html format can be open in any browser once downloaded, without needing access to Databricks
    - Download and uncompress the .zip file and you will see the individual notebooks in .html extension
  
For the code to work, you need to create a directory named `mlops2021` and place all the individual notebooks underneath, shown in the screenshot: 

<img width="309" alt="Screen Shot 2021-06-12 at 11 21 30 AM" src="https://user-images.githubusercontent.com/12697839/121782684-72ef6780-cb70-11eb-9341-152138237efc.png">

