{"cells":[{"cell_type":"markdown","source":["# Setup Notebook"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70b11902-b5cf-4ad5-b4b5-59464e744dd7"}}},{"cell_type":"markdown","source":["Use [notebook scoped libraries]() to install required dependency versions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1317248-3fe1-4301-9256-c32320e2662c"}}},{"cell_type":"code","source":["%pip install mlflow==1.16.0 scipy==1.6.3 seaborn==0.11.1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e52dcce7-fe2a-4bc7-80f8-ac42dbae512c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting mlflow==1.16.0\n  Using cached mlflow-1.16.0-py3-none-any.whl (14.2 MB)\nCollecting scipy==1.6.3\n  Using cached scipy-1.6.3-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\nCollecting seaborn==0.11.1\n  Using cached seaborn-0.11.1-py3-none-any.whl (285 kB)\nRequirement already satisfied: protobuf&gt;=3.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (3.13.0)\nRequirement already satisfied: sqlparse&gt;=0.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (0.4.1)\nRequirement already satisfied: requests&gt;=2.17.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (2.24.0)\nRequirement already satisfied: databricks-cli&gt;=0.8.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (0.14.3)\nRequirement already satisfied: numpy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.19.2)\nCollecting sqlalchemy\n  Using cached SQLAlchemy-1.4.18-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\nRequirement already satisfied: pyyaml in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (5.4.1)\nRequirement already satisfied: gunicorn; platform_system != &#34;Windows&#34; in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (20.0.4)\nRequirement already satisfied: pytz in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (2020.5)\nProcessing /root/.cache/pip/wheels/69/6f/b4/2087abb1172ae32c58e366dc09746de46a72b0e9fb2c022920/prometheus_flask_exporter-0.18.2-py3-none-any.whl\nRequirement already satisfied: docker&gt;=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (4.4.4)\nRequirement already satisfied: gitpython&gt;=2.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (3.1.12)\nRequirement already satisfied: cloudpickle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.6.0)\nProcessing /root/.cache/pip/wheels/9d/de/6d/ca8d461ec29e010b1267d7353d0b058819770f7680bb9360e4/alembic-1.4.1-py2.py3-none-any.whl\nRequirement already satisfied: pandas in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.1.5)\nRequirement already satisfied: entrypoints in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (0.3)\nRequirement already satisfied: querystring-parser in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.2.4)\nRequirement already satisfied: Flask in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.1.2)\nRequirement already satisfied: click&gt;=7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (7.1.2)\nRequirement already satisfied: matplotlib&gt;=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from seaborn==0.11.1) (3.2.2)\nRequirement already satisfied: six&gt;=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from protobuf&gt;=3.6.0-&gt;mlflow==1.16.0) (1.15.0)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from protobuf&gt;=3.6.0-&gt;mlflow==1.16.0) (50.3.1.post20201107)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.16.0) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.16.0) (1.25.11)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.16.0) (2020.12.5)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.16.0) (3.0.4)\nRequirement already satisfied: tabulate&gt;=0.7.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow==1.16.0) (0.8.7)\nCollecting greenlet!=0.4.17; python_version &gt;= &#34;3&#34;\n  Using cached greenlet-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\nRequirement already satisfied: prometheus-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from prometheus-flask-exporter-&gt;mlflow==1.16.0) (0.10.1)\nRequirement already satisfied: websocket-client&gt;=0.32.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from docker&gt;=4.0.0-&gt;mlflow==1.16.0) (0.57.0)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from gitpython&gt;=2.1.0-&gt;mlflow==1.16.0) (4.0.7)\nRequirement already satisfied: python-editor&gt;=0.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.16.0) (1.0.4)\nRequirement already satisfied: python-dateutil in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.16.0) (2.8.1)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.16.0) (1.1.3)\nRequirement already satisfied: Werkzeug&gt;=0.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from Flask-&gt;mlflow==1.16.0) (1.0.1)\nRequirement already satisfied: Jinja2&gt;=2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from Flask-&gt;mlflow==1.16.0) (2.11.2)\nRequirement already satisfied: itsdangerous&gt;=0.24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from Flask-&gt;mlflow==1.16.0) (1.1.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.1) (1.3.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.1) (2.4.7)\nRequirement already satisfied: cycler&gt;=0.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.1) (0.10.0)\nRequirement already satisfied: smmap&lt;5,&gt;=3.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&gt;=2.1.0-&gt;mlflow==1.16.0) (3.0.5)\nRequirement already satisfied: MarkupSafe&gt;=0.9.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from Mako-&gt;alembic&lt;=1.4.1-&gt;mlflow==1.16.0) (1.1.1)\nInstalling collected packages: greenlet, sqlalchemy, prometheus-flask-exporter, alembic, mlflow, scipy, seaborn\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.5.2\n    Uninstalling scipy-1.5.2:\n      Successfully uninstalled scipy-1.5.2\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.10.0\n    Uninstalling seaborn-0.10.0:\n      Successfully uninstalled seaborn-0.10.0\nSuccessfully installed alembic-1.4.1 greenlet-1.1.0 mlflow-1.16.0 prometheus-flask-exporter-0.18.2 scipy-1.6.3 seaborn-0.11.1 sqlalchemy-1.4.18\nPython interpreter will be restarted.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting mlflow==1.16.0\n  Using cached mlflow-1.16.0-py3-none-any.whl (14.2 MB)\nCollecting scipy==1.6.3\n  Using cached scipy-1.6.3-cp38-cp38-manylinux1_x86_64.whl (27.2 MB)\nCollecting seaborn==0.11.1\n  Using cached seaborn-0.11.1-py3-none-any.whl (285 kB)\nRequirement already satisfied: protobuf&gt;=3.6.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (3.13.0)\nRequirement already satisfied: sqlparse&gt;=0.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (0.4.1)\nRequirement already satisfied: requests&gt;=2.17.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (2.24.0)\nRequirement already satisfied: databricks-cli&gt;=0.8.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (0.14.3)\nRequirement already satisfied: numpy in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.19.2)\nCollecting sqlalchemy\n  Using cached SQLAlchemy-1.4.18-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\nRequirement already satisfied: pyyaml in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (5.4.1)\nRequirement already satisfied: gunicorn; platform_system != &#34;Windows&#34; in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (20.0.4)\nRequirement already satisfied: pytz in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (2020.5)\nProcessing /root/.cache/pip/wheels/69/6f/b4/2087abb1172ae32c58e366dc09746de46a72b0e9fb2c022920/prometheus_flask_exporter-0.18.2-py3-none-any.whl\nRequirement already satisfied: docker&gt;=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (4.4.4)\nRequirement already satisfied: gitpython&gt;=2.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (3.1.12)\nRequirement already satisfied: cloudpickle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.6.0)\nProcessing /root/.cache/pip/wheels/9d/de/6d/ca8d461ec29e010b1267d7353d0b058819770f7680bb9360e4/alembic-1.4.1-py2.py3-none-any.whl\nRequirement already satisfied: pandas in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.1.5)\nRequirement already satisfied: entrypoints in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (0.3)\nRequirement already satisfied: querystring-parser in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.2.4)\nRequirement already satisfied: Flask in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (1.1.2)\nRequirement already satisfied: click&gt;=7.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from mlflow==1.16.0) (7.1.2)\nRequirement already satisfied: matplotlib&gt;=2.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from seaborn==0.11.1) (3.2.2)\nRequirement already satisfied: six&gt;=1.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from protobuf&gt;=3.6.0-&gt;mlflow==1.16.0) (1.15.0)\nRequirement already satisfied: setuptools in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from protobuf&gt;=3.6.0-&gt;mlflow==1.16.0) (50.3.1.post20201107)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.16.0) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.16.0) (1.25.11)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.16.0) (2020.12.5)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow==1.16.0) (3.0.4)\nRequirement already satisfied: tabulate&gt;=0.7.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow==1.16.0) (0.8.7)\nCollecting greenlet!=0.4.17; python_version &gt;= &#34;3&#34;\n  Using cached greenlet-1.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\nRequirement already satisfied: prometheus-client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from prometheus-flask-exporter-&gt;mlflow==1.16.0) (0.10.1)\nRequirement already satisfied: websocket-client&gt;=0.32.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from docker&gt;=4.0.0-&gt;mlflow==1.16.0) (0.57.0)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from gitpython&gt;=2.1.0-&gt;mlflow==1.16.0) (4.0.7)\nRequirement already satisfied: python-editor&gt;=0.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.16.0) (1.0.4)\nRequirement already satisfied: python-dateutil in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.16.0) (2.8.1)\nRequirement already satisfied: Mako in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from alembic&lt;=1.4.1-&gt;mlflow==1.16.0) (1.1.3)\nRequirement already satisfied: Werkzeug&gt;=0.15 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from Flask-&gt;mlflow==1.16.0) (1.0.1)\nRequirement already satisfied: Jinja2&gt;=2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from Flask-&gt;mlflow==1.16.0) (2.11.2)\nRequirement already satisfied: itsdangerous&gt;=0.24 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from Flask-&gt;mlflow==1.16.0) (1.1.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.1) (1.3.0)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.1) (2.4.7)\nRequirement already satisfied: cycler&gt;=0.10 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from matplotlib&gt;=2.2-&gt;seaborn==0.11.1) (0.10.0)\nRequirement already satisfied: smmap&lt;5,&gt;=3.0.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&gt;=2.1.0-&gt;mlflow==1.16.0) (3.0.5)\nRequirement already satisfied: MarkupSafe&gt;=0.9.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-c33c1aac-a115-4abb-8469-aad8d74ac723/lib/python3.8/site-packages (from Mako-&gt;alembic&lt;=1.4.1-&gt;mlflow==1.16.0) (1.1.1)\nInstalling collected packages: greenlet, sqlalchemy, prometheus-flask-exporter, alembic, mlflow, scipy, seaborn\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.5.2\n    Uninstalling scipy-1.5.2:\n      Successfully uninstalled scipy-1.5.2\n  Attempting uninstall: seaborn\n    Found existing installation: seaborn 0.10.0\n    Uninstalling seaborn-0.10.0:\n      Successfully uninstalled seaborn-0.10.0\nSuccessfully installed alembic-1.4.1 greenlet-1.1.0 mlflow-1.16.0 prometheus-flask-exporter-0.18.2 scipy-1.6.3 seaborn-0.11.1 sqlalchemy-1.4.18\nPython interpreter will be restarted.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Imports"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4364d033-951b-4f64-b367-2efea8f691b9"}}},{"cell_type":"code","source":["from delta.tables import DeltaTable\nimport tempfile\nimport os\nimport numpy as np\nimport pandas as pd\nimport pyspark.sql.functions as F\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nfrom mlflow.exceptions import RestException\nfrom mlflow.models.signature import ModelSignature\nfrom mlflow.types.schema import Schema, ColSpec"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87d7e752-67ef-40b1-85e1-3e08b4ac0455"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Notebook Configs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cbacdb7-2c85-40f2-9c65-f2abebc50551"}}},{"cell_type":"code","source":["# Get Databricks workspace username\nusername = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply(\"user\")\nprint(username)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10e58f60-7b4a-4c82-b431-4603c06d8a93"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">chengyin.eng@databricks.com\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">chengyin.eng@databricks.com\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Set variables to use for reading/writing tmp artifacts and datasets\nproject_home_dir = f\"/Users/{username}/mlops2021/\"\nproject_local_tmp_dir = \"/dbfs\" + project_home_dir + \"tmp/\"\ndata_project_dir = f\"{project_home_dir}data/\"\n\n# Remove Data Project directory if exists - want to ensure there are no existing versions of Delta tables there\ndbutils.fs.rm(data_project_dir, True)\ndbutils.fs.mkdirs(data_project_dir)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2c76eee-17e3-40a3-a25f-7750408068f6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Set MLflow experiment path - require that we have created the folder DAIS_2021 in our user workspace \nworkspace_project_home = f\"/Users/{username}/mlops2021\"\n\nexperiment_path = workspace_project_home + \"/airbnb_hawaii\"\nmlflow.set_experiment(experiment_path)\n\n# Get the unique experiment ID from the provided\nexperiment_id = mlflow.get_experiment_by_name(experiment_path).experiment_id\n\n# Define model name for MLflow Registry\nregistry_model_name = \"airbnb_hawaii\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86531278-a5a0-4a0c-8aa6-f20883ece120"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">INFO: &#39;/Users/chengyin.eng@databricks.com/mlops2021/airbnb_hawaii&#39; does not exist. Creating a new experiment\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">INFO: &#39;/Users/chengyin.eng@databricks.com/mlops2021/airbnb_hawaii&#39; does not exist. Creating a new experiment\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Data Configs"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffec405a-3999-4547-a6e9-dbde2560091c"}}},{"cell_type":"code","source":["# Path to Delta table of cleaned Airbnb Hawaii dataset\n# This dataset was downloaded from http://insideairbnb.com/get-the-data.html \n# The cleaned (Delta) dataset can be found in the Google drive associated with bit.ly/dais_2021_drifting_away\n# You can use the Databricks CLI https://docs.databricks.com/dev-tools/cli/index.html to upload this directory\n# Download locally from the Google Drive, set up the CLI and use `dbfs cp -r airbnb-hawaii.delta dbfs:/path/to/dir/airbnb-hawaii.delta`\nraw_delta_path = \"dbfs:/dais-2021/airbnb-hawaii.delta\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b3cd0840-00dc-4e79-8002-25d8a016e53f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Paths to write/read data from\nmonth_0_delta_path = data_project_dir + \"month_0_delta\"\n\n# Two separate Data paths - one with error data/one without\nmonth_1_error_delta_path = data_project_dir + \"month_1_error_delta\"\nmonth_1_fixed_delta_path = data_project_dir + \"month_1_fixed_delta\"\n\nmonth_2_delta_path = data_project_dir + \"month_2_delta\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa474054-0d0f-42ce-a239-528669b7c088"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Dataset Creation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d188fb3a-97f2-4ed5-a145-7c71e9e58a99"}}},{"cell_type":"markdown","source":["We will be creating synthetic errors in our datasets, simulating a case where we have 3 months of consecutive data, the first month being the dataset we use to train and \"deploy\" our first model, followed by 2 consecutive months where we have simulated different forms of drift."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d24e2264-64a1-40ad-a913-580a79f24f6b"}}},{"cell_type":"code","source":["# Load full dataset and subset to specified columns\nairbnb_df = spark.read.format(\"delta\").load(raw_delta_path)\n\ntarget_col = \"price\"\nnum_cols = [\"accommodates\",\n            \"bedrooms\",\n            \"beds\",\n            \"minimum_nights\",\n            \"number_of_reviews\",\n            \"number_of_reviews_ltm\",\n            \"review_scores_rating\"]\ncat_cols = [\"host_is_superhost\",\n            \"neighbourhood_cleansed\",\n            \"property_type\",\n            \"room_type\"]\n\ncols_to_keep = [target_col] + num_cols + cat_cols\nairbnb_df = airbnb_df.select(cols_to_keep)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74cd9189-3a59-4403-b736-b9a751c865a6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Creating the data for each scenario"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"585fcd07-7b5e-4e8e-ae25-de7dde43c2df"}}},{"cell_type":"code","source":["# The suffix of the variables used will correspond to the month they are intended to be used for\ndf_0, df_1, df_2 = airbnb_df.randomSplit(weights=[1.0, 1.0, 1.0], seed=42)\n\ndf_0.write.format(\"delta\").save(month_0_delta_path)\ndf_1.write.format(\"delta\").save(month_1_fixed_delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f45092a-f5f9-4046-9828-0a547ca88f31"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Scenario 1 Creation\n* Certain neighbourhoods are missing their `neighbourhood_cleansed` entries\n* The upstream data generation procedure for `review_scores_rating` has resulted in the previously 0-100 rating system being altered to a 0-5 star system"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8e770b8-9d64-45a0-89c1-12e313f97ab9"}}},{"cell_type":"code","source":["# Create a DataFrame which takes the clean data and introduces simulated errors into the dataset\ndf_1_err = (df_1\n             .withColumn(\"neighbourhood_cleansed\",                                 # Simulate some neighbourhood entires as being cleansed incorrectly  \n                         F.when((F.col(\"neighbourhood_cleansed\") == \"Primary Urban Center\") |  \n                                (F.col(\"neighbourhood_cleansed\") == \"Kihei-Makena\") | \n                                (F.col(\"neighbourhood_cleansed\") == \"Lahaina\") | \n                                (F.col(\"neighbourhood_cleansed\") == \"North Kona\"), F.lit(None)).otherwise(F.col(\"neighbourhood_cleansed\")))\n            .fillna(0, subset=[\"review_scores_rating\"])                            # Fill missing ratings with 0\n            .withColumn(\"review_scores_rating\", F.col(\"review_scores_rating\")/20)  # Scale ratings to be between 0 and 5\n            )\n\ndf_1_err.write.format(\"delta\").save(month_1_error_delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd0bdde2-1aa9-4e9c-9c05-4036fb93441e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Scenario 2:\n* The new month of data contains listing entries recorded during peak vacation season. As a result, the price for every listing has been increased."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"88982a0d-891c-4213-811e-0564e12b5f9f"}}},{"cell_type":"code","source":["df_2_err = df_2.withColumn(\"price\", F.col(\"price\") + (2*F.col(\"price\")*F.rand(seed=42)))\n\ndf_2_err.write.format(\"delta\").save(month_2_delta_path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5ca3a93-f8de-4a49-9063-9153764b9d7c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Utility Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ed022ef-02b0-4675-af3b-258005f32d01"}}},{"cell_type":"markdown","source":["### MLflow Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b38f38be-ffd6-4fcf-9951-f41f61c01b1d"}}},{"cell_type":"code","source":["\"\"\"\nMLflow Registry Clean Up \n\"\"\"\n\ndef cleanup_registered_model(registry_model_name):\n  \"\"\"\n  Utilty function to delete a registered model in MLflow model registry.\n  To delete a model in the model registry all model versions must first be archived.\n  This function thus first archives all versions of a model in the registry prior to\n  deleting the model\n  \n  :param registry_model_name: (str) Name of model in MLflow model registry\n  \"\"\"\n  client = MlflowClient()\n\n  filter_string = f'name=\"{registry_model_name}\"'\n\n  model_versions = client.search_model_versions(filter_string=filter_string)\n  \n  if len(model_versions) > 0:\n    print(f\"Deleting following registered model: {registry_model_name}\")\n    \n    # Move any versions of the model to Archived\n    for model_version in model_versions:\n      try:\n        model_version = client.transition_model_version_stage(name=model_version.name,\n                                                              version=model_version.version,\n                                                              stage=\"Archived\")\n      except mlflow.exceptions.RestException:\n        pass\n\n    client.delete_registered_model(registry_model_name)\n    \n  else:\n    print(\"No registered models to delete\")    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"427b1eba-8922-43ae-a170-7380e64b20bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Remove this registered model if it already exists\ncleanup_registered_model(registry_model_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f6abed51-1319-4d94-8d00-ca91745f09c6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\"\"\"\nMLflow Tracking Utility Methods\n\"\"\"\n\ndef get_delta_version(delta_path):\n  \"\"\"\n  Function to get the most recent version of a Delta table give the path to the Delta table\n  \n  :param delta_path: (str) path to Delta table\n  :return: Delta version (int)\n  \"\"\"\n  # DeltaTable is the main class for programmatically interacting with Delta tables\n  delta_table = DeltaTable.forPath(spark, delta_path)\n  # Get the information of the latest commits on this table as a Spark DataFrame. \n  # The information is in reverse chronological order.\n  delta_table_history = delta_table.history() \n  \n  # Retrieve the lastest Delta version - this is the version loaded when reading from delta_path\n  delta_version = delta_table_history.first()[\"version\"]\n  \n  return delta_version\n\n\ndef create_summary_stats_pdf(pdf):\n  \"\"\"\n  Create a pandas DataFrame of summary statistics for a provided pandas DataFrame.\n  Involved calling .describe on pandas DataFrame provided and additionally add\n  median values and a count of null values for each column.\n  \n  :param pdf: pandas DataFrame\n  :return: pandas DataFrame of sumary statistics for each column\n  \"\"\"\n  stats_pdf = pdf.describe(include=\"all\")\n\n  # Add median values row\n  median_vals = pdf.median()\n  stats_pdf.loc[\"median\"] = median_vals\n\n  # Add null values row\n  null_count = pdf.isna().sum()\n  stats_pdf.loc[\"null_count\"] = null_count\n\n  return stats_pdf\n\n\ndef log_summary_stats_pdf_as_csv(pdf):\n  \"\"\"\n  Log summary statistics pandas DataFrame as a csv file to MLflow as an artifact\n  \"\"\"\n  temp = tempfile.NamedTemporaryFile(prefix=\"summary_stats_\", suffix=\".csv\")\n  temp_name = temp.name\n  try:\n    pdf.to_csv(temp_name)\n    mlflow.log_artifact(temp_name, \"summary_stats.csv\")\n  finally:\n    temp.close() # Delete the temp file\n    \n    \ndef load_summary_stats_pdf_from_run(run, local_tmp_dir):\n  \"\"\"\n  Given an MLflow run, download the summary stats csv artifact to a local_tmp_dir and load the\n  csv into a pandas DataFrame\n  \n  :param run: mlflow.entities.run.Run\n  :param local_tmp_dir: (str) path to a local filesystem tmp directory\n  :return pandas DataFrame containing statistics computed during training\n  \"\"\"\n  # Use MLflow clitent to download the csv file logged in the artifacts of a run to a local tmp path\n  client = MlflowClient()\n  if not os.path.exists(local_tmp_dir):\n      os.mkdir(local_tmp_dir)\n  local_path = client.download_artifacts(run.info.run_id, \"summary_stats.csv\", local_tmp_dir)\n  print(f\"Summary stats artifact downloaded in: {local_path}\")\n  \n  # Load the csv into a pandas DataFrame\n  summary_stats_path = local_path + \"/\" + os.listdir(local_path)[0]\n  summary_stats_pdf = pd.read_csv(summary_stats_path, index_col=\"Unnamed: 0\")\n  \n  return summary_stats_pdf \n\n\ndef load_delta_table_from_run(run):\n  \"\"\"\n  Given an MLflow run, load the Delta table which was used for that run,\n  using the path and version tracked at tracking time.\n  Note that by default Delta tables only retain a commit history for 30 days, meaning\n  that previous versions older than 30 days will be deleted by default. This property can\n  be updated using the Delta table property delta.logRetentionDuration.\n  For more information, see https://docs.databricks.com/delta/delta-batch.html#data-retention\n  \n  :param run: mlflow.entities.run.Run\n  :return: Spark DataFrame\n  \"\"\"\n  delta_path = run.data.params[\"delta_path\"]\n  delta_version = run.data.params[\"delta_version\"]\n  print(f\"Loading Delta table from path: {delta_path}; version: {delta_version}\")\n  df = spark.read.format(\"delta\").option(\"versionAsOf\", delta_version).load(delta_path)\n  \n  return df  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6df9ef5c-4051-4272-bd9e-9978bf4ff85d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\"\"\"\nMLflow Registry Utility Methods\n\"\"\"\n\ndef transition_model(model_version, stage):\n    \"\"\"\n    Transition a model to a specified stage in MLflow Model Registry using the associated \n    mlflow.entities.model_registry.ModelVersion object.\n\n    :param model_version: mlflow.entities.model_registry.ModelVersion. ModelVersion object to transition\n    :param stage: (str) New desired stage for this model version. One of \"Staging\", \"Production\", \"Archived\" or \"None\"\n\n    :return: A single mlflow.entities.model_registry.ModelVersion object\n    \"\"\"\n    client = MlflowClient()\n    \n    model_version = client.transition_model_version_stage(\n        name=model_version.name,\n        version=model_version.version,\n        stage=stage,\n        archive_existing_versions=True\n    )\n\n    return model_version  \n  \n\ndef fetch_model_version(registry_model_name, stage=\"Staging\"):\n    \"\"\"\n    For a given registered model, return the MLflow ModelVersion object\n    This contains all metadata needed, such as params logged etc\n\n    :param registry_model_name: (str) Name of MLflow Registry Model\n    :param stage: (str) Stage for this model. One of \"Staging\" or \"Production\"\n\n    :return: mlflow.entities.model_registry.ModelVersion\n    \"\"\"\n    client = MlflowClient()\n    filter_string = f'name=\"{registry_model_name}\"'\n    registered_model = client.search_registered_models(filter_string=filter_string)[0]\n\n    if len(registered_model.latest_versions) == 1:\n        model_version = registered_model.latest_versions[0]\n\n    else:\n        model_version = [model_version for model_version in registered_model.latest_versions if model_version.current_stage == stage][0]\n\n    return model_version\n\n  \ndef get_run_from_registered_model(registry_model_name, stage=\"Staging\"):\n    \"\"\"\n    Get Mlflow run object from registered model\n\n    :param registry_model_name: (str) Name of MLflow Registry Model\n    :param stage: (str) Stage for this model. One of \"Staging\" or \"Production\"\n\n    :return: mlflow.entities.run.Run\n    \"\"\"\n    model_version = fetch_model_version(registry_model_name, stage)\n    run_id = model_version.run_id\n    run = mlflow.get_run(run_id)\n\n    return run  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ee97650-cc02-4757-878c-88de6f68d2f0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Model Training Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a088b611-7af9-46d3-84e1-3e00fb4bff4b"}}},{"cell_type":"code","source":["def create_sklearn_rf_pipeline(model_params, seed=42):\n  \"\"\"\n  Create the sklearn pipeline required for the RandomForestRegressor.\n  We compose two components of the pipeline separately - one for numeric cols, one for categorical cols\n  These are then combined with the final RandomForestRegressor stage, which uses the model_params dict\n  provided via the args. The unfitted pipeline is returned.\n  \n  For a robust pipeline in practice, one should also have a pipeline stage to add indicator columns for those features\n  which have been imputed. This can be useful to encode information about those instances which have been imputed with\n  a given value. We refrain from doing so here to simplify the pipeline, and focus on the overall workflow.\n  \n  :param model_params: (dict) Dictionary of model parameters to pass into sklearn RandomForestRegressor\n  :param seed : (int) Random seed to set via random_state arg in RandomForestRegressor\n \n  :return: sklearn pipeline\n  \"\"\"\n  # Create pipeline component for numeric Features\n  numeric_transformer = Pipeline(steps=[\n      (\"imputer\", SimpleImputer(strategy='median'))])\n\n  # Create pipeline component for categorical Features\n  categorical_transformer = Pipeline(steps=[\n      (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n      (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n\n  # Combine numeric and categorical components into one preprocessor pipeline\n  # Use ColumnTransformer to apply the different preprocessing pipelines to different subsets of features\n  # Use selector (make_column_selector) to select which subset of features to apply pipeline to\n  preprocessor = ColumnTransformer(transformers=[\n      (\"numeric\", numeric_transformer, selector(dtype_exclude=\"category\")),\n      (\"categorical\", categorical_transformer, selector(dtype_include=\"category\"))\n  ])\n\n  pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor),\n                             (\"rf\", RandomForestRegressor(random_state=seed, \n                                                          **model_params))\n                            ])\n  \n  return pipeline\n\n\ndef train_sklearn_rf_model(run_name, delta_path, model_params, misc_params, seed=42):\n  \"\"\"\n  Function to trigger training and evaluation of an sklearn RandomForestRegressor model.\n  Parameters, metrics and artifacts are logged to MLflow during this process.\n  Return the MLflow run object \n  \n  :param run_name: (str) name to give to MLflow run\n  :param delta_path: (str) path to Delta table to use as input data\n  :param model_params: (dict) Dictionary of model parameters to pass into sklearn RandomForestRegressor\n  :param misc_params: (dict) Dictionary of params to use \n  \n  :return: mlflow.entities.run.Run  \n  \"\"\"  \n  with mlflow.start_run(run_name=run_name) as run:\n\n    # Enable MLflow autologging\n    mlflow.autolog(log_input_examples=True, silent=True)\n    \n    # Load Delta table from delta_path\n    df = spark.read.format(\"delta\").load(delta_path)   \n    # Log Delta path and version\n    mlflow.log_param(\"delta_path\", delta_path)\n    delta_version = get_delta_version(delta_path)\n    mlflow.log_param(\"delta_version\", delta_version)\n    \n    # Track misc parameters used in pipeline creation (preprocessing) as json artifact\n    mlflow.log_dict(misc_params, \"preprocessing_params.json\")\n    target_col = misc_params[\"target_col\"]  \n    num_cols = misc_params[\"num_cols\"]    \n    cat_cols = misc_params[\"cat_cols\"]    \n\n    # Convert Spark DataFrame to pandas, as we will be training an sklearn model\n    pdf = df.toPandas() \n    # Convert all cat cols to category dtype\n    for c in cat_cols:\n        pdf[c] = pdf[c].astype(\"category\")    \n    \n    # Create summary statistics pandas DataFrame and log as a csv to MLflow\n    summary_stats_pdf = create_summary_stats_pdf(pdf)\n    log_summary_stats_pdf_as_csv(summary_stats_pdf)  \n    \n    # Track number of total instances and \"month\"\n    num_instances = pdf.shape[0]\n    mlflow.log_param(\"num_instances\", num_instances)  # Log number of instances\n    mlflow.log_param(\"month\", misc_params[\"month\"])   # Log month number\n    \n    # Split data\n    X = pdf.drop([misc_params[\"target_col\"], \"month\"], axis=1)\n    y = pdf[misc_params[\"target_col\"]]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n    # Track train/test data info as params\n    num_training = X_train.shape[0]\n    mlflow.log_param(\"num_training_instances\", num_training)\n    num_test = X_test.shape[0]\n    mlflow.log_param(\"num_test_instances\", num_test)\n\n    # Fit sklearn pipeline with RandomForestRegressor model\n    rf_pipeline = create_sklearn_rf_pipeline(model_params)\n    rf_pipeline.fit(X_train, y_train)\n    # Specify data schema which the model will use as its ModelSignature\n    input_schema = Schema([\n      ColSpec(\"integer\", \"accommodates\"),\n      ColSpec(\"integer\", \"bedrooms\"),\n      ColSpec(\"integer\", \"beds\"),\n      ColSpec(\"integer\", \"number_of_reviews\"),\n      ColSpec(\"integer\", \"number_of_reviews_ltm\"),\n      ColSpec(\"integer\", \"minimum_nights\"),\n      ColSpec(\"integer\", \"review_scores_rating\"),\n      ColSpec(\"string\", \"host_is_superhost\"),\n      ColSpec(\"string\", \"neighbourhood_cleansed\"),\n      ColSpec(\"string\", \"property_type\"),\n      ColSpec(\"string\", \"room_type\")\n    ])\n    output_schema = Schema([ColSpec(\"double\")])\n    signature = ModelSignature(input_schema, output_schema)\n    mlflow.sklearn.log_model(rf_pipeline, \"model\", signature=signature)\n\n    # Evaluate the model\n    predictions = rf_pipeline.predict(X_test)\n    test_mse = mean_squared_error(y_test, predictions) \n    r2 = r2_score(y_test, predictions)\n    mlflow.log_metrics({\"test_mse\": test_mse,\n                       \"test_r2\": r2})\n\n  return run"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54c24494-dda8-4585-aa67-f62cb06a450d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"training_setup","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":430509414919504}},"nbformat":4,"nbformat_minor":0}
